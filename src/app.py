import os
import pickle
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
from flask import Flask, render_template, request
from transformers import TFBertForSequenceClassification, BertTokenizer
from tensorflow.keras import Input


# Global constants
MAX_LEN = 100
labels = ["Negative", "Neutral", "Positive"]

app = Flask(__name__)

# --- Paths for LSTM model and tokenizer ---
LSTM_MODEL_PATH = os.path.join("flask_sentiment_app", "model", "lstm_model.h5")
LSTM_TOKENIZER_PATH = os.path.join("flask_sentiment_app", "model", "tokenizer.pkl")

# --- Path for BERT model (generated by train_bert.py) ---
BERT_MODEL_DIR = os.path.join("flask_sentiment_app", "model", "bert_model")

# Check for file existence
if not os.path.exists(LSTM_MODEL_PATH):
    raise FileNotFoundError(f"LSTM model not found at {LSTM_MODEL_PATH}")
if not os.path.exists(LSTM_TOKENIZER_PATH):
    raise FileNotFoundError(f"LSTM tokenizer not found at {LSTM_TOKENIZER_PATH}")
if not os.path.exists(BERT_MODEL_DIR):
    raise FileNotFoundError(f"BERT model directory not found at {BERT_MODEL_DIR}")

# Load the model and tokenizer
lstm_model = load_model(LSTM_MODEL_PATH)
with open(LSTM_TOKENIZER_PATH, "rb") as f:
    lstm_tokenizer = pickle.load(f)

# --- Load BERT Model and its Tokenizer ---
bert_model = TFBertForSequenceClassification.from_pretrained(BERT_MODEL_DIR)
bert_tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_DIR)

def preprocess_for_lstm(text: str):
    """
    Uses the LSTM tokenizer to convert text into a padded sequence.
    """
    seq = lstm_tokenizer.texts_to_sequences([text])
    padded = pad_sequences(seq, maxlen=MAX_LEN, padding="post", truncating="post")
    return padded

def preprocess_for_bert(text: str):
    """
    Uses the BERT tokenizer to convert text into inputs for the BERT model.
    Pads/truncates sequences to a maximum length.
    """
    # Use "max_length" padding and truncation; we use MAX_LEN here (or adjust as needed)
    inputs = bert_tokenizer(text, return_tensors="tf", padding="max_length", truncation=True, max_length=MAX_LEN)
    return inputs

@app.route("/", methods=["GET", "POST"])
def index():
    prediction = None
    if request.method == "POST":
        tweet = request.form.get("tweet", "").strip()
        model_choice = request.form.get("model_choice", "LSTM").strip()
        if tweet:
            if model_choice == "LSTM":
                processed = preprocess_for_lstm(tweet)
                pred = lstm_model.predict(processed)
                sentiment_idx = np.argmax(pred)
                prediction = f"[LSTM] Predicted Sentiment: {labels[sentiment_idx]}"
            elif model_choice == "BERT":
                inputs = preprocess_for_bert(tweet)
                outputs = bert_model(inputs)
                # If the model was trained with from_logits=True, apply softmax to get probabilities.
                probabilities = tf.nn.softmax(outputs.logits, axis=-1).numpy()
                sentiment_idx = np.argmax(probabilities)
                prediction = f"[BERT] Predicted Sentiment: {labels[sentiment_idx]}"
            else:
                prediction = "Invalid model choice."
    return render_template("index.html", prediction=prediction)

if __name__ == "__main__":
    # Run the Flask app from the terminal
    app.run(debug=True, host="0.0.0.0", port=5000)
